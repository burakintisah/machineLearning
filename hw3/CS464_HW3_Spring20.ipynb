{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5yghM_BMmkQ"
   },
   "source": [
    "<h1><center>CS 464</center></h1>\n",
    "<h1><center>Introduction to Machine Learning</center></h1>\n",
    "<h1><center>Spring 2020</center></h1>\n",
    "<h1><center>Homework 3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ds6L6MMXMmkR"
   },
   "source": [
    "<h3><center>Due: May 19, 2020 23:55 (GMT+3)</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCsDGpxqMmkT"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    This homework contains both written and programming questions about neural networks. You should implement programming questions on this notebook. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question. <b>Any other programming enviroment will NOT be accepted.</b>\n",
    "    </li>\n",
    "    <li>\n",
    "    For questions that you need to plot, your plot results have to be included in both cell output. For written questions, you may provide them either as comments in code cells or as seperate text cells. \n",
    "    </li>\n",
    "    <li>\n",
    "    You are <b>ONLY ALLOWED</b> to use libraries given below:\n",
    "        <ul>\n",
    "        <i>>google.colab.drive</i><br>\n",
    "         <i>>pandas</i><br>\n",
    "         <i>>numpy</i><br>\n",
    "         <i>>libraries included in Python standard library (time, os, sys etc.)</i><br>\n",
    "         <i>>libraries included in PyTorch framework (torch, torchvision etc.)</i><br>\n",
    "         <i>>PIL.Image</i><br>\n",
    "         <i>>matplotlib</i>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    It is <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
    "    </li>\n",
    "    <li>\n",
    "    You will submit only a single compressed file for this homework. Compress your notebook(\".ipynb\") and model (\".pth\") files as a gzipped TAR file or a ZIP file with the name CS464_HW3_Section#_Firstname_Lastname. Do not use any Turkish letters for any of your files including code files and model files. Upload your homework to the related section on Moodle.\n",
    "    </li>\n",
    "    <li>\n",
    "    This is an individual assignment for each student. That is, you are NOT allowed to share your workwith your classmates.</li>\n",
    "    <li> \n",
    "    If you do not follow the submission routes, deadlines and specifications, it will lead to a significant grade deduction.\n",
    "    </li>\n",
    "    <li> \n",
    "    For any question regarding this assignment, contact <b>ilayda.beyreli@bilkent.edu.tr</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cE7TyUOpMmkU"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDBMnGMFlc50"
   },
   "source": [
    "You may use both anaconda or pip to install PyTorch to your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wbzHKKbMmkV"
   },
   "source": [
    "### Anaconda Installation\n",
    "\n",
    "<ul>\n",
    "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
    "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
    "</ul>\n",
    "\n",
    "#### Creation of Virtual Environment\n",
    "\n",
    "<ul>\n",
    "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
    "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
    "    <li>Activate your virtual environment<br>\n",
    "        <i>> source activate HW3</i></li>\n",
    "    <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
    "         <i>> jupyter notebook</i>\n",
    "</ul>\n",
    "\n",
    "\n",
    "#### Pytorch Installation with Anaconda\n",
    "\n",
    "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
    "<li>> source activate HW3</li>\n",
    "\n",
    "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
    "<ul>\n",
    "<li>For MacOS:<br>\n",
    "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
    "</li>\n",
    "<li>For Linux:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
    "</li>\n",
    "<li>For Windows:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
    "</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oV6Va_SmV5i"
   },
   "source": [
    "###Pip3 Installation\n",
    "<ul>\n",
    "    <li>Download pip3 from https://pip.pypa.io/en/stable/installing/</li>\n",
    "    <li>If you are using Windows, you may need to add Python to your enviroment variables. You may use the following tutorial to install Python and pip.\n",
    "    https://phoenixnap.com/kb/how-to-install-python-3-windows</li>\n",
    "</ul>\n",
    "\n",
    "#### PyTorch Installation with Pip\n",
    "<ul>\n",
    "<li>For MacOS:<br>\n",
    "    <i>> pip3 install torch torchvision</i>\n",
    "</li>\n",
    "<li>For Linux:<br>\n",
    "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i>\n",
    "</li>\n",
    "<li>For Windows:<br>\n",
    "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i><br>\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGRjkX0pMmkY"
   },
   "source": [
    "## Question 1 Decison Trees [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BcwAjaoMmkZ"
   },
   "source": [
    "![Dataset](https://drive.google.com/uc?export=view&id=1k6-cF0zIJ2TFtey1WZ8Y5wevWcDtKckd)\n",
    "\n",
    "  Consider the dataset shown above. You will train a binary decision tree using information gain as the splitting criteria. Consider the following stopping criteria (ie. early pruning criteria): If the entropy of a node is below a predefined threshold, $T$, stop splitting that node, and set it as a leaf.<br>\n",
    "\n",
    " Reminder: You are not allowed to put scanned images. Hence, you need to use drawing tools such as draw.io, Paint, Microsoft Powerpoint etc.<br>\n",
    "\n",
    "**a) [10 pts]** Draw the decision tree that is trained on this dataset without any pruning (i.e. $T=0$). Use <b>ID3 algorithm</b> and <b>entropy</b> as your impurity measure to construct your tree. Show your calculation for each split decision and justify your answer clearly.<br>\n",
    "\n",
    "**b) [5 pts]** Draw the decision boundaries resulting from the decision tree you have drawn for part a.<br>\n",
    "\n",
    "**c) [5 pts]** Assume we have a very large balanced dataset with 2 classes. Draw a hypothetical plot, which shows training and test errors (Y-axis) as $T$ changes from 1 to 0 (X-axis). Explain your rationale explicitly.<br>\n",
    "\n",
    "**d) [5 pts]** Is ID3 algorithm optimal? If yes, provide an intuition of optimality. If not, explain the reason explicitly.<br>\n",
    "\n",
    "**e) [5 pts]** If you used Gini Index as your impurity measure instead of entropy, would you obtain the same tree as in part a? Explain your reasoning clearly.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmN0va8GMmka"
   },
   "source": [
    "## Question 2 [70 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3mspgJlMmkb"
   },
   "source": [
    "In this question you are asked to perform binary classification on Ocular Disease Recognition, ODIR5k, dataset. First, you will implement a three-layer neural network, and then a 3 layer convolutional neural network (CNN) to classify retinal images of 3500 patiens as either \"normal\" or \"abnormal/disease\".<br><br>\n",
    "The dataset has been preprocessed in such a way that the right and the left retinal images from the same patient are combined, downsized and stored as H:128 X W:256 RGB images. The label for each patient is also given to you in another .xlsx file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgzLrMkW8xxU"
   },
   "source": [
    "### 2.1. Multi Layer Perceptron (MLP) [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yzlbHxVMmkc"
   },
   "source": [
    "#### Data Loader [7pts]\n",
    "\n",
    "An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below to create your custom \"OcularDataset\" class which will be able to load your dataset. <br><br>\n",
    "Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)** sets.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the \"get_dataset\" function. Here, you are also supposed to flatten the image into a vector (also to grayscale) to be compatible with MLP. Note that the pixel values also needs to be normalized to [0,1] range.\n",
    "<br><br>\n",
    "\n",
    "Hint: The dataset is not normalized and your results will heavily depend on your input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKA_ylTx6SVM"
   },
   "outputs": [],
   "source": [
    "class OcularDataset(Dataset):\n",
    "    \n",
    "    # TODO:\n",
    "    # Define constructor for AnimalDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, data, label, **kwargs):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    # '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    # '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        _x = self.data[index]\n",
    "        _y = self.label[index]\n",
    "        return _x, _y\n",
    "\n",
    "\n",
    "def get_dataset(root):\n",
    "    # TODO: \n",
    "    # Read dataset files\n",
    "    # Construct training, validation and test sets\n",
    "    # Normalize & flatten datasets\n",
    "    \n",
    "    # I assumed that my root file contains a file with images folder and label.xlsx excel file.\n",
    "    # According to this assumption I have done the followings\n",
    "    file = root + '\\labels.xlsx'\n",
    "    xl = pd.ExcelFile(file)\n",
    "    labels = xl.parse('Sheet1')\n",
    "    img_dir = root + '\\images'\n",
    "    \n",
    "    # I have found the length of image data\n",
    "    all_images = os.listdir(img_dir)\n",
    "    total = len (all_images)\n",
    "    \n",
    "    # find the length of train, val, test data sets\n",
    "    train_len = int(tot * 5 / 7)\n",
    "    val_len = int(tot * 1 / 7)\n",
    "    test_len = int(tot * 1 / 7)\n",
    "    \n",
    "    \n",
    "    # according to these numbers I have divided the datas into train, val and test data sets.\n",
    "    # taking train data\n",
    "    train_data = []\n",
    "    train_gt = []\n",
    "    for i in range (0,train_len):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        # opening image in greyscale\n",
    "        vec = Image.open(img_address).convert(\"L\")\n",
    "        img = np.array(vec).flatten()\n",
    "        # in order to normalize\n",
    "        img = img / 255\n",
    "        train_data.append(img)\n",
    "        train_gt.append(labels[1][i])\n",
    "    \n",
    "    # taking val data\n",
    "    val_data = []\n",
    "    val_gt = []\n",
    "    start = train_len\n",
    "    end = train_len + val_len\n",
    "    for i in range (train_len,train_len+val_len):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        # opening image in greyscale\n",
    "        vec = Image.open(img_address).convert(\"L\")\n",
    "        img = np.array(vec).flatten()\n",
    "        # in order to normalize\n",
    "        img = img / 255\n",
    "        val_data.append(img)\n",
    "        val_gt.append(labels[1][i])\n",
    "\n",
    "    # taking test data\n",
    "    test_data = []\n",
    "    test_gt = []\n",
    "    start = train_len + val_len\n",
    "    for i in range (start,total):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        # opening image in greyscale\n",
    "        vec = Image.open(img_address).convert(\"L\")\n",
    "        img = np.array(vec).flatten()\n",
    "        # in order to normalize\n",
    "        img = img / 255\n",
    "        test_data.append(img)\n",
    "        test_gt.append(labels[1][i])\n",
    "    \n",
    "    # Training data should be in type float\n",
    "    train_data = np.array(train_data, dtype=float)\n",
    "    val_data = np.array(val_data, dtype=float)\n",
    "    test_data = np.array(test_data, dtype=float)\n",
    "    \n",
    "    # there were some bugs and trying to make labels long  \n",
    "    train_gt = np.array(train_gt, dtype= np.long)\n",
    "    val_gt = np.array(val_gt, dtype= np.long)\n",
    "    test_gt = np.array(test_gt, dtype= np.long)\n",
    "\n",
    "    # creating Dataset objects\n",
    "    train_dataset = OcularDataset (data = train_data, label= train_gt )\n",
    "    val_dataset = OcularDataset (data = val_data, label = val_gt )\n",
    "    test_dataset = OcularDataset (data = test_data, label=test_gt )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMfyonGRMmke"
   },
   "source": [
    "#### Neural Network [4 pts]\n",
    "\n",
    "Now, implement your three hidden layer neural network. FNet class will represent your neural network. The layer descriptions are as follows:<ul>\n",
    "    <i>> Input layer will have ReLU activation. You should decide the number of input neurons.</i><br>\n",
    "    <i>> First hidden layer will have 1024 neuros with ReLU activation </i><br>\n",
    "    <i>> Second hidden layer will have 256 neuros with ReLU activation </i><br>\n",
    "    <i> You should decide the number of output neurons and pick a proper activation function for the output layer. </i><br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed imports for following sections\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# why this returns false\n",
    "print(torch.cuda.device_count())\n",
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adM6pBIp6cIa"
   },
   "outputs": [],
   "source": [
    "class FNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, layer1, layer2, **kwargs): \n",
    "    # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "        super().__init__()\n",
    "        first_layer = layer1\n",
    "        second_layer = layer2\n",
    "        # hidden layer 1\n",
    "        self.hidden_layer1 = nn.Linear(32768, first_layer)\n",
    "        # hidden layer 2\n",
    "        self.hidden_layer2 = nn.Linear(first_layer, second_layer)\n",
    "        # in the homework description you have provided there should be 3 hiden layers but \n",
    "        # I though that output layer is not a hidden layer \n",
    "        # so I named it as output layer\n",
    "        self.output_layer = nn.Linear(second_layer, 4)\n",
    "     \n",
    "    def forward(self, X): \n",
    "    # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here\n",
    "        sze = X.size(0)\n",
    "        X = F.relu(self.hidden_layer1(X))\n",
    "        X = F.relu(self.hidden_layer2(X))\n",
    "        # Flatten the tensor \n",
    "        X = X.view(sze, -1) \n",
    "        X = self.output_layer(X)\n",
    "        # for numerical stability as our TA said\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4-GhJe0Mmkf"
   },
   "source": [
    "#### Training [10 pts]\n",
    "\n",
    "Complete the code snippet below to train your network. You need to carefully select the appropriate loss function and tune hyper-parameters. Use SGD optimizer for this question.<br>\n",
    "So far, you should have created three dataset splits for train, validation and test. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Save training loss and training accuracy of each iteration (each batch) and also save validation loss and accuracy at each epoch to use them in the next part for plotting.<br>\n",
    "Your model is going to run upto at most 100 epochs. Pick the best model so far as your final model and save this model as a \".pth\" file. <br>\n",
    "Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlkS5jVR6kNb"
   },
   "outputs": [],
   "source": [
    "''' Train your network for a one epoch '''\n",
    "#  CAREFUL I have swaped with main and train/test \n",
    "\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        \n",
    "        # Using autograd Variable\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        \n",
    "        # zero out the gradient in order to do the parameter update correctly.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # giving data to model\n",
    "        output = model(data.float())\n",
    "               \n",
    "        # calculatin the loss with \n",
    "        loss = F.nll_loss(output, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset), 100. * batch_idx / len(loader), loss.item()))\n",
    "        \n",
    "        '''\n",
    "        print ('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(trainloader), \n",
    "               batch_time=batch_time,\n",
    "               data_time=data_time, \n",
    "               loss=loss.item(),\n",
    "               acc=accuracies))\n",
    "        '''\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader): # you are free to change parameters\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            \n",
    "            # Using autograd Variable\n",
    "            data, target = Variable(data, volatile=True), Variable(labels)\n",
    "            # giving data to model\n",
    "            output = model(data.float())\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, labels.long(), size_average=False).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.long().data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(loader.dataset)\n",
    "    acc = (100. * correct / len(loader.dataset))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(loader.dataset),\n",
    "            100. * correct / len(loader.dataset)))\n",
    "    '''print('Time {batch_time.avg:.3f}\\t'\n",
    "          'Accu {acc.avg:.4f}\\t'.format(\n",
    "           batch_time=batch_time, \n",
    "           acc=accuracies))'''\n",
    "        \n",
    "    return acc, test_loss\n",
    "        \n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#HINT: note that your training time should not take many days.\n",
    "\n",
    "#TODO:\n",
    "#Pick your hyper parameters\n",
    "max_epoch = 10\n",
    "train_batch = 64\n",
    "test_batch = 16\n",
    "\n",
    "learning_rate = 0.0008\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# Main START HERE\n",
    "\n",
    "def main(): # you are free to change parameters\n",
    "\n",
    "    # Create train dataset loader\n",
    "    # Create validation dataset loader\n",
    "    # Create test dataset loader\n",
    "    # initialize your GENet neural network\n",
    "    # define your loss function\n",
    "    \n",
    "    root = os.getcwd()\n",
    "    print(\"Current Directory\")\n",
    "    print(root)\n",
    "    # os.listdir(root)\n",
    "    \n",
    "    print(\"Datas are loading..\")\n",
    "    training, val, testing = get_dataset(root)\n",
    "    \n",
    "    \n",
    "    trainloader =  torch.utils.data.DataLoader(dataset=training, batch_size=train_batch, shuffle=True)\n",
    "    \n",
    "    val_loader =  torch.utils.data.DataLoader(dataset=val, batch_size=train_batch, shuffle=True)\n",
    "    \n",
    "    test_loader =  torch.utils.data.DataLoader(dataset=testing, batch_size=test_batch, shuffle=False)\n",
    "    \n",
    "    # initializing my neural network\n",
    "    print(\"Neural Network Initiliazing..\")\n",
    "    model = FNet(1024, 256)\n",
    "\n",
    "    # Build loss function ??\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # you can play with momentum and weight_decay parameters as well\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) \n",
    "    \n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "\n",
    "    print(\"Training Start..\")\n",
    "    # just to be sure best acc = 0 every time\n",
    "    best_acc = 0\n",
    "    criterion = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        train(epoch, model, criterion, optimizer, trainloader)\n",
    "        # these acc and loss are calculated from our validation dataset\n",
    "        acc, loss = test(model, val_loader)\n",
    "        if acc > best_acc:\n",
    "            # need to change best acc every time new accuracy is better than before \n",
    "            best_acc = acc\n",
    "            torch.save(model, root + \"/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWJZLiIiMmkj"
   },
   "source": [
    "# Plotting Your Results [4 pts]\n",
    "\n",
    "You need to provide two distinct plots, one demonstrating training and validation losses in y axis and iteration in the x axis and the other demonstrating training and validation accuracies in the y axis and iteration  in the x axis. <br><br>\n",
    "Please note that we need these plots to see if your model behaves as expected. Therefore, you may lose additional points if you do not provide these plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDfGUr10Mmkj"
   },
   "outputs": [],
   "source": [
    "# write your code in this cell to plot your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7v8am0hMmkm"
   },
   "source": [
    "#### Testing [5 pts]\n",
    "\n",
    "Test your final, i.e. best, model on your test set. Calculate confusion matrix, F1 score, precision and recall values and report these findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aq6YqUadMmkn"
   },
   "outputs": [],
   "source": [
    "# write your code in this cell to test your best model with the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-J6f1QPMmk3"
   },
   "source": [
    "### 2.2. Convolutional Neural Network (CNN) [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssXrFEtjMmk4"
   },
   "source": [
    "#### Data Loader [5 pts]\n",
    "\n",
    "In this part, you will train a CNN for the same problem. Again, the pixel values need to be normalized to [0,1] range. Please do **not** change images to grayscale this time. First, implement the data loader (OcularDataset). You have to divide the files into three sets which are <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)**.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the \"get_dataset\" function.<br> You may use your data loader from the previous sections with propoer modifications. Note that this time you do **not** need to flatten the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLpZ2tCnMmk4"
   },
   "outputs": [],
   "source": [
    "class OcularDataset(Dataset):\n",
    "    # TODO:\n",
    "    # Define constructor for SVHNDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, **kwargs): # you are free to change parameters\n",
    "        self.data = kwargs[\"data_samples\"]\n",
    "        self.gt = kwargs[\"gt\"]\n",
    "        \n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        __x = self.data[index]\n",
    "        __y = self.gt[index]\n",
    "        return _x, _y\n",
    "\n",
    "        \n",
    "def get_dataset(root):\n",
    "    # TODO: \n",
    "    # Read dataset files\n",
    "    # Construct training, validation and test sets\n",
    "    # Normalize datasets\n",
    "    file = root + '\\labels.xlsx'\n",
    "    xl = pd.ExcelFile(file)\n",
    "    labels = xl.parse('Sheet1')\n",
    "    img_dir = root + '\\images'\n",
    "    \n",
    "    # I have found the length of image data\n",
    "    all_images = os.listdir(img_dir)\n",
    "    total = len (all_images)\n",
    "    \n",
    "    # find the length of train, val, test data sets\n",
    "    train_len = int(tot * 5 / 7)\n",
    "    val_len = int(tot * 1 / 7)\n",
    "    test_len = int(tot * 1 / 7)\n",
    "    \n",
    "    \n",
    "    # according to these numbers I have divided the datas into train, val and test data sets.\n",
    "    # taking train data\n",
    "    train_data = []\n",
    "    train_gt = []\n",
    "    for i in range (0,train_len):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        \n",
    "        # opening image\n",
    "        vec = Image.open(img_address)\n",
    "        img = np.array(vec)\n",
    "        \n",
    "        # in order to normalize\n",
    "        img_1d = img.reshape(-1)\n",
    "        img_1d = img_1d / 255\n",
    "        normalized_img = img_1d.reshape(128,256,3)\n",
    "        # print(normalized_img.shape)\n",
    "        \n",
    "        train_data.append(normalized_img)\n",
    "        train_gt.append(labels[1][i])\n",
    "    \n",
    "    # taking val data\n",
    "    val_data = []\n",
    "    val_gt = []\n",
    "    start = train_len\n",
    "    end = train_len + val_len\n",
    "    for i in range (train_len,train_len+val_len):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        \n",
    "        # opening image \n",
    "        vec = Image.open(img_address)\n",
    "        img = np.array(vec)\n",
    "        \n",
    "        # in order to normalize\n",
    "        img_1d = img.reshape(-1)\n",
    "        img_1d = img_1d / 255\n",
    "        normalized_img = img_1d.reshape(128,256,3)\n",
    "        # print(normalized_img.shape)\n",
    "        \n",
    "        val_data.append(normalized_img)\n",
    "        val_gt.append(labels[1][i])\n",
    "\n",
    "    # taking test data\n",
    "    test_data = []\n",
    "    test_gt = []\n",
    "    start = train_len + val_len\n",
    "    for i in range (start,total):\n",
    "        img_name = all_images[i]\n",
    "        img_address = os.path.join(img_dir,img_name)\n",
    "        \n",
    "        # opening image in greyscale\n",
    "        vec = Image.open(img_address)\n",
    "        img = np.array(vec).flatten()\n",
    "        \n",
    "        # in order to normalize\n",
    "        img_1d = img.reshape(-1)\n",
    "        img_1d = img_1d / 255\n",
    "        normalized_img = img_1d.reshape(128,256,3)\n",
    "        # print(normalized_img.shape)\n",
    "        \n",
    "        test_data.append(normalized_img)\n",
    "        test_gt.append(labels[1][i])\n",
    "    \n",
    "    # Training data should be in type float\n",
    "    train_data = np.array(train_data, dtype=float)\n",
    "    val_data = np.array(val_data, dtype=float)\n",
    "    test_data = np.array(test_data, dtype=float)\n",
    "    \n",
    "    train_dataset = OcularDataset (data_samples = train_data, gt= train_gt )\n",
    "    val_dataset = OcularDataset (data_samples = val_data, gt = val_gt )\n",
    "    test_dataset = OcularDataset (data_samples = test_data, gt=test_gt )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-RsQD3EMmk8"
   },
   "source": [
    "#### Convolutional Neural Network [8 pts]\n",
    "\n",
    "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
    "1. <i>> 8 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br> \n",
    "    <i>> ReLU </i><br>\n",
    "2. <i>> 16 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>\n",
    "    <i>> ReLU </i><br>\n",
    "    <i>> MaxPool 2 x 2 </i><br>   \n",
    "3. <i>> 32 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>\n",
    "    <i>> ReLU </i><br>\n",
    "    <i>> MaxPool 2 x 2 </i><br>\n",
    "\n",
    "As a classification layer, you need to add only one more fully-connected layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGiD0Y_oMmk9"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(3, 8, kernel_size=3)\n",
    "        self.cnn2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.cnn3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.fully_connected_layer = nn.Linear(,)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "     \n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here\n",
    "        in_size = X.size(0)\n",
    "        X = F.relu(self.cnn1(X))\n",
    "        X = F.relu(self.max_pool(self.cnn2(X)))\n",
    "        X = F.relu(self.max_pool(self.cnn3(X)))\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        X = self.fully_connected_layer(X)\n",
    "        return F.log_softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioU02PmPMmlA"
   },
   "source": [
    "#### Training and Testing [17 pts]\n",
    "\n",
    "Now, train your network. You need to select the appropriate loss function and your hyper-parameters.<br>\n",
    "Make sure to shuffle the samples in the training split.<br>\n",
    " Plot the training and validation loss for each iteration. Also plot the training  and validation accuracy as another figure.<br>\n",
    "  Your model is going to run upto the \"max_epoch\" parameter. Pick the best model as your final model and save this model as a \".pth\" file. Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies. <br>\n",
    "  Report the test performance change (In terms of accuracy, F1 score, precision and recall) between MLP and CNN and explain the reason for this change explicitly, if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swWSqCgnMmlD"
   },
   "outputs": [],
   "source": [
    "# HINT: note that your training time should not take many days.\n",
    "\n",
    "# TODO:\n",
    "# Pick your hyper parameters\n",
    "# max_epoch = \n",
    "# train_batch = \n",
    "# test_batch = \n",
    "# learning_rate =\n",
    "\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "def main(): # you are free to change parameters\n",
    "\n",
    "    # Create train dataset loader\n",
    "    # Create validation dataset loader\n",
    "    # Create test dataset loader\n",
    "    # initialize your GENet neural network\n",
    "    # define your loss function\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    \n",
    "    #for epoch in range(max_epoch):\n",
    "    #    train(epoch, model, criterion, optimizer, trainloader)\n",
    "    #    acc = test(model, val_loader)\n",
    "    #    if acc > best_acc:\n",
    "    #       torch.save(model, best_path)\n",
    "    \n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        \n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(trainloader), \n",
    "               batch_time=batch_time,\n",
    "               data_time=data_time, \n",
    "               loss=losses,\n",
    "               acc=accuracies))\n",
    "\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader): # you are free to change parameters\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            \n",
    "        print('Time {batch_time.avg:.3f}\\t'\n",
    "              'Accu {acc.avg:.4f}\\t'.format(\n",
    "               batch_time=batch_time, \n",
    "               acc=accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCDpcMJTHD0h"
   },
   "source": [
    "### 2.3 Interpretation [10 pts.]\n",
    "\n",
    "Explicitly discuss the results that you have obtained in Question 2. <ul>\n",
    "    > Among MLP and CNN , which one do you think is better? <br>\n",
    "    > What are the weaknesses and strengths of each method?<br>\n",
    "    > Why do we use max pooling layers for CNN? What would happen if we used average pooling instead? <br>\n",
    "    > How can we interpret the weights of convolutional layers? <br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7q8jg7nfFUe-"
   },
   "source": [
    "##References\n",
    "\n",
    "Ocular Disease Recognition - ODIR5k Dataset (https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS464_HW3_Spring20.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
