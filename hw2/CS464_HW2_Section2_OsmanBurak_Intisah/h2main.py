# -*- coding: utf-8 -*-
"""ML- homework2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kJ_Q8o5RNEBF61vbBmKe32NA7rrmdd8c

# **SVM & Logistic Regression & PCA**
"""

import os
from scipy.io import loadmat  # for reading .mat files
import numpy as np
import time


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

root = 'C:/Users/asus/Desktop/4. Sınıf 1. Dönem/CS 464 - ML/hw2'

"""# Reading data both q1 and q2"""

q1 = os.path.join(root, 'q1_dataset.mat')
q2 = os.path.join(root, 'q2_dataset.mat')

q1_dataset = loadmat(q1)
q2_dataset = loadmat(q2)

"""**Extracting the variables as we need them**"""

inception_features_train = q1_dataset['inception_features_train']
inception_features_test = q1_dataset['inception_features_test']
hog_features_train = q1_dataset['hog_features_train']
hog_features_test = q1_dataset['hog_features_test']
superclass_labels_train = q1_dataset['superclass_labels_train']
superclass_labels_test = q1_dataset['superclass_labels_test']
subclass_labels_train = q1_dataset['subclass_labels_train']
subclass_labels_test = q1_dataset['subclass_labels_test']

# The shape of the variables match with shapes given in homework description
# All of them are numpy.ndarray

"""# Question 1 
**Logistic Regression Class**
"""


class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iter=1000, batch_size=25):
        self.learning_rate = learning_rate
        self.num_iter = num_iter
        self.batch_size = batch_size
        self.lambd = 0.2

    # function to create a list containing mini-batches
    def create_mini_batches(self, X, y, batch_size):
        mini_batches = []
        data = np.hstack((X, y))
        np.random.shuffle(data)
        n_minibatches = data.shape[0] // batch_size
        i = 0
        for i in range(n_minibatches):
            mini_batch = data[i * batch_size:(i + 1) * batch_size, :]
            X_mini = mini_batch[:, :-1]
            Y_mini = mini_batch[:, -1].reshape((-1, 1))
            mini_batches.append((X_mini, Y_mini))

        return mini_batches

    # Logistic regression expression
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    # for classifying all of the test set
    def test_classify(self, test_classify, treshold, weights):
        prob = self.sigmoid(np.dot(test_classify, weights))
        prob[prob >= treshold] = int(1)
        prob[prob < treshold] = int(0)
        return prob.flatten()

    def log_likelihood(self, x, y, weights):
        scores = np.dot(x, weights)
        ll = np.sum(np.dot(y.T, scores) - np.log(1 + np.exp(scores)))
        return ll

    def gradient_ascent(self, x, y):
        # Initializing all weights to random numbers drawn from a Gaussian distribution N(0, 0.01)
        mean, sigma = 0, 0.01  # mean and standard deviation
        weights = np.random.uniform(mean, sigma, size=(x.shape[1], 1))

        for i in range(self.num_iter):

            # first we need to find the mini batches
            mini_batches = self.create_mini_batches(x, y, self.batch_size)

            for mini_batch in mini_batches:
                x_mini, y_mini = mini_batch
                h = self.sigmoid(np.dot(x_mini, weights))
                # Updating
                out = y_mini - h
                gradient = np.dot(x_mini.T, out)
                weights = weights + self.learning_rate * gradient

            if i % 100 == 0:
                log_like = self.log_likelihood(x, y, weights)
                print('Accuracy (Maximum Likelihood Estimation): {} in iteration {} of {}'.format(log_like, i,
                                                                                                  self.num_iter))

        return weights

    def stochastic_gradient_ascent(self, x, y):
        # Initializing all weights to random numbers drawn from a Gaussian distribution N(0, 0.01)
        mean, sigma = 0, 0.01  # mean and standard deviation
        weights = np.random.uniform(mean, sigma, size=(x.shape[1], 1))

        for i in range(self.num_iter):

            # first we need to find the mini batches
            mini_batches = self.create_mini_batches(x, y, self.batch_size)

            for mini_batch in mini_batches:
                x_mini, y_mini = mini_batch

                h = self.sigmoid(np.dot(x_mini, weights))

                # Updating
                out = y_mini - h
                gradient = np.dot(x_mini.T, out)
                weights = weights + self.learning_rate * gradient

            if i % 100 == 0:
                log_like = self.log_likelihood(x, y, weights)
                print('Accuracy (Maximum Likelihood Estimation): {} in iteration {} of {}'.format(log_like, i,
                                                                                                  self.num_iter))

        return weights

    def full_batch_gradient_ascent(self, x, y):
        # Initializing all weights to random numbers drawn from a Gaussian distribution N(0, 0.01)
        mean, sigma = 0, 0.01  # mean and standard deviation
        weights = np.random.uniform(mean, sigma, size=(x.shape[1], 1))

        for i in range(self.num_iter):
            h = self.sigmoid(np.dot(x, weights))

            # Updating
            out = y - h
            gradient = np.dot(x.T, out)
            weights = weights + self.learning_rate * gradient

            if i % 100 == 0:
                log_like = self.log_likelihood(x, y, weights)
                # If you want to print all weights for each 100 iteration you need to comment out below code
                # print("Weights .. in iteration {} of {} \n {}".format(i, self.num_iter, weights.reshape(324,1)))
                print('Accuracy (Maximum Likelihood Estimation): {} in iteration {} of {}'.format(log_like, i,
                                                                                                  self.num_iter))

        return weights

    # For finding the results that are asked !! False positive- true positive etc...
    def report_result(self, predictions, label):
        predictions = predictions.reshape(-1, 1)
        label = label
        tp = 0
        fp = 0
        tn = 0
        fn = 0
        for i in range(len(predictions)):
            if label[i] == predictions[i] == 1:
                tp += 1
            if predictions[i] == 1 and label[i] != predictions[i]:
                fp += 1
            if label[i] == predictions[i] == 0:
                tn += 1
            if predictions[i] == 0 and label[i] != predictions[i]:
                fn += 1

        accuracy = (tp + tn) / (fp + fn + tp + tn)
        precision = tp / (tp + fp)
        recall = tp / (tp + fn)
        npv = tn / (tn + fn)
        fpr = fp / (fp + tn)
        fdr = fp / (fp + tp)
        f1_score = 2 * (recall * precision) / (recall + precision)
        f2_score = (5 * precision * recall) / (4 * precision * recall)
        conf_mat = [[tp, fp], [fn, tn]]
        # print ("Total num : " + str (tp+tn+fn+fp))
        print("Accuracy : {}".format(accuracy))
        print("Precision : {}".format(precision))
        print("Recall : {}".format(recall))
        print("NPV : {}".format(npv))
        print("FPR : {}".format(fpr))
        print("FDR : {}".format(fdr))
        print("F1 Score : {}".format(f1_score))
        print("F2 Score: {}".format(f2_score))
        print("Confusion Matrix: {}".format(conf_mat))
        return accuracy


"""# 1.1 Mini-batch gradient ascent algorithm with batch size = 25"""

# Commented out IPython magic to ensure Python compatibility.
learning_rates = [1e-6, 1e-5, 1e-3, 1e-1, 1]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.gradient_ascent ( np.mat(hog_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end-start))
    # doing predictions according to the weights found
    predictions = model.test_classify(hog_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)
    best_list.append(accuracy)
    print()

max_ind = best_list.index(max(best_list))
best_lr_mini_hog = learning_rates[max_ind]
print("Best learning rate for mini batch trained with hog: {} ".format(best_lr_mini_hog))

"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
learning_rates = [1e-6, 1e-5, 1e-3, 1e-1, 1]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.gradient_ascent ( np.mat(inception_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    # doing predictions according to the weights found
    predictions = model.test_classify(inception_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)
    best_list.append(accuracy)
    print()

max_ind = best_list.index(max(best_list))
best_lr_mini_inc = learning_rates[max_ind]
print("Best learning rate for mini batch trained with neural network: {} ".format(best_lr_mini_inc))

"""# 1.1 Stochastic gradient ascent"""

# Commented out IPython magic to ensure Python compatibility.
learning_rates = [1e-6, 1e-5, 1e-3, 1e-1, 1]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.stochastic_gradient_ascent ( np.mat(hog_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    # doing predictions according to the weights found
    predictions = model.test_classify(hog_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)
    best_list.append(accuracy)
    print()

max_ind = best_list.index(max(best_list))
best_lr_sch_hog = learning_rates[max_ind]
print("Best learning rate for stochastic trained with hog: {} ".format(best_lr_sch_hog))

"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
learning_rates = [1e-6, 1e-5, 1e-3, 1e-1, 1]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.stochastic_gradient_ascent ( np.mat(inception_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    # doing predictions according to the weights found
    predictions = model.test_classify(inception_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)
    best_list.append(accuracy)
    print()

max_ind = best_list.index(max(best_list))
best_lr_sch_inc = learning_rates[max_ind]
print("Best learning rate for stochastic trained with neural net: {} ".format(best_lr_sch_inc))

"""# 1.2 Full batch gradient ascent"""

# Commented out IPython magic to ensure Python compatibility.
import operator

# learning_rates = [ best_lr_mini_hog, best_lr_sch_hog]
learning_rates = [1e-5]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.full_batch_gradient_ascent( np.mat(hog_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    indexed = list(enumerate(result))
    top_10 = sorted(indexed, key=operator.itemgetter(1))[-10:]
    indexs = list(reversed([i for i, v in top_10]))
    print("10 most important feature indices: \n{}".format(indexs))

    # doing predictions according to the weights found
    predictions = model.test_classify(hog_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)

"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
# learning_rates = [ best_lr_mini_inc, best_lr_sch_inc]
learning_rates = [0.1]
best_list = []

for i in learning_rates:
    print("When learning rate is {} .. ".format(i))
    model = LogisticRegression(learning_rate=i, num_iter=1000, batch_size=25)

    # weights will be found according to learning rate and
    start = time.time()
    result = model.full_batch_gradient_ascent( np.mat(inception_features_train), np.mat(superclass_labels_train) )
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    indexed = list(enumerate(result))
    top_10 = sorted(indexed, key=operator.itemgetter(1))[-10:]
    indexs = list(reversed([i for i, v in top_10]))
    print("10 most important feature indices: \n{}".format(indexs))

    # doing predictions according to the weights found
    predictions = model.test_classify(inception_features_test, 0.5, result)

    # finding results
    accuracy = model.report_result(predictions, superclass_labels_test)

"""# Question 1.3
**Superclass Classication Using SVM**
"""

inception_features_train = q1_dataset['inception_features_train']
inception_features_test = q1_dataset['inception_features_test']
hog_features_train = q1_dataset['hog_features_train']
hog_features_test = q1_dataset['hog_features_test']
superclass_labels_train = q1_dataset['superclass_labels_train']
superclass_labels_test = q1_dataset['superclass_labels_test']

from sklearn.model_selection import StratifiedKFold
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from statistics import mean


class class_svm:
    # def __init__ (self):

    def stratified_k_fold(self, features, labels, k):
        result = []
        skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)
        for train_index, test_index in skf.split(features, labels):
            # print("TRAIN:", len(train_index), "TEST:", len(test_index))
            X_train, X_test = features[train_index], features[test_index]
            y_train, y_test = labels[train_index], labels[test_index]
            result.append([X_train, X_test, y_train, y_test])

        return np.asarray(result)

    def train_soft_margin_svm(self, fold, cs):

        X_train, X_test, y_train, y_test = fold
        y_train = y_train.flatten()

        all_acc = []
        all_pred = []
        all_recall = []
        all_f1 = []

        # Create a svm Classifier
        for c in cs:
            # Linear Kernel
            clf = svm.SVC(kernel='linear', C=c)
            # Train the model using the training sets
            clf.fit(X_train, y_train)
            # Predict the response for test dataset
            y_pred = clf.predict(X_test)

            # Model Accuracy: how often is the classifier correct?
            acc = metrics.accuracy_score(y_test, y_pred)
            all_acc.append(acc)
            # Model Precision: what percentage of positive tuples are labeled as such?
            pred = metrics.precision_score(y_test, y_pred)
            all_pred.append(pred)
            # Model Recall: what percentage of positive tuples are labelled as such?
            recall = metrics.recall_score(y_test, y_pred)
            all_recall.append(recall)
            # Model F1 Score:
            f1 = metrics.f1_score(y_test, y_pred)
            all_f1.append(f1)

            print("When c= {}, Accuracy: {}".format(c, acc))
            print("When c= {}, Precision: {}".format(c, pred))
            print("When c= {}, Recall: {}".format(c, recall))
            print("When c= {}, F1 Score: {}".format(c, f1))

        if len(cs) != 1:
            return all_acc

        else:
            cnf_matrix = confusion_matrix(y_test, y_pred)
            print(cnf_matrix)

        print()

    def train_hard_margin_svm(self, fold, gamas):
        X_train, X_test, y_train, y_test = fold
        y_train = y_train.flatten()

        all_acc = []
        all_pred = []
        all_recall = []
        all_f1 = []

        # Create a svm Classifier
        for gama in gamas:
            # RBF Kernel
            clf = svm.SVC(kernel='rbf', gamma=gama, C=1e11)
            # Train the model using the training sets
            clf.fit(X_train, y_train)
            # Predict the response for test dataset
            y_pred = clf.predict(X_test)

            # Model Accuracy: how often is the classifier correct?
            acc = metrics.accuracy_score(y_test, y_pred)
            all_acc.append(acc)
            # Model Precision: what percentage of positive tuples are labeled as such?
            pred = metrics.precision_score(y_test, y_pred)
            all_pred.append(pred)
            # Model Recall: what percentage of positive tuples are labelled as such?
            recall = metrics.recall_score(y_test, y_pred)
            all_recall.append(recall)
            # Model F1 Score:
            f1 = metrics.f1_score(y_test, y_pred)
            all_f1.append(f1)

            print("When gamma= {}, Accuracy: {}".format(gama, acc))
            print("When gamma= {}, Precision: {}".format(gama, pred))
            print("When gamma= {}, Recall: {}".format(gama, recall))
            print("When gamma= {}, F1 Score: {}".format(gama, f1))

        if len(gamas) != 1:
            return all_acc

        else:
            cnf_matrix = confusion_matrix(y_test, y_pred)
            print(cnf_matrix)
        print()

    def train_soft_margin_svm_last(self, fold, gamas, cs):

        # Test and training data,label
        X_train, X_test, y_train, y_test = fold
        y_train = y_train.flatten()

        all_acc = []
        all_pred = []
        all_recall = []
        all_f1 = []

        for c in cs:

            for gama in gamas:
                # RBF Kernel
                clf = svm.SVC(kernel='rbf', gamma=gama, C=c)
                # Train the model using the training sets
                clf.fit(X_train, y_train)
                # Predict the response for test dataset
                y_pred = clf.predict(X_test)

                # Model Accuracy: how often is the classifier correct?
                acc = metrics.accuracy_score(y_test, y_pred)
                all_acc.append(acc)
                # Model Precision: what percentage of positive tuples are labeled as such?
                pred = metrics.precision_score(y_test, y_pred)
                all_pred.append(pred)
                # Model Recall: what percentage of positive tuples are labelled as such?
                recall = metrics.recall_score(y_test, y_pred)
                all_recall.append(recall)
                # Model F1 Score:
                f1 = metrics.f1_score(y_test, y_pred)
                all_f1.append(f1)

                print("When gamma= {}, c = {}, Accuracy: {}".format(gama, c, acc))
                print("When gamma= {}, c = {}, Precision: {}".format(gama, c, pred))
                print("When gamma= {}, c = {}, Recall: {}".format(gama, c, recall))
                print("When gamma= {}, c = {}, F1 Score: {}".format(gama, c, f1))
                print()

        if len(gamas) != 1:
            return all_acc

        else:
            cnf_matrix = confusion_matrix(y_test, y_pred)
            print(cnf_matrix)

        print()


"""# 1.4 Train a soft margin SVM model with linear kernel. 
Tune C hyper-parameter of the model using 5-fold cross validation on the training dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
cs = [1e-2, 1e-1, 1, 1e1, 1e2]
model = class_svm()
result = model.stratified_k_fold(features=hog_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    f1 = model.train_soft_margin_svm(result[i], cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(f1)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
best = ind_arr[0][0]
print("Best of the C value: {} \n".format(cs[best]))
print(avg_f1)

"""**After selecting optimal C value**"""

# Commented out IPython magic to ensure Python compatibility.
c = [cs[best]]
model = class_svm()
fold = [hog_features_train, hog_features_test, superclass_labels_train, superclass_labels_test]
start = time.time()
model.train_soft_margin_svm(fold, c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
cs = [1e-2, 1e-1, 1, 1e1, 1e2]
model = class_svm()
result = model.stratified_k_fold(features=inception_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    f1 = model.train_soft_margin_svm(result[i], cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(f1)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
best = ind_arr[0][0]
print("Best of the C value: {} \n".format(cs[best]))
print(avg_f1)

"""**After selecting optimal C value**"""

# Commented out IPython magic to ensure Python compatibility.
c = [cs[best]]
model = class_svm()
fold = [inception_features_train, inception_features_test, superclass_labels_train, superclass_labels_test]
start = time.time()
model.train_soft_margin_svm(fold, c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# 1.5 Train a hard margin SVM with radial basis function (rbf) kernel."""

# Commented out IPython magic to ensure Python compatibility.
gamas = [2 ** -4, 2 ** -3, 2 ** -2, 2 ** -1, 2 ** 0, 2 ** 1, 2 ** 6]
print(gamas)
model = class_svm()
result = model.stratified_k_fold(features=hog_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    f1 = model.train_hard_margin_svm(result[i], gamas)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(f1)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
best = ind_arr[0][0]
print("Best of the Gamma value: {} \n".format(gamas[best]))
print(avg_f1)

"""**After selecting optimal Gamma value**"""

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[best]]
model = class_svm()
fold = [hog_features_train, hog_features_test, superclass_labels_train, superclass_labels_test]
start = time.time()
model.train_hard_margin_svm(fold, gama)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))

"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
gamas = [2 ** -4, 2 ** -3, 2 ** -2, 2 ** -1, 2 ** 0, 2 ** 1, 2 ** 6]
print(gamas)
model = class_svm()
result = model.stratified_k_fold(features=inception_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    f1 = model.train_hard_margin_svm(result[i], gamas)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(f1)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
best = ind_arr[0][0]
print("Best of the Gamma value: {} \n".format(gamas[best]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[best]]
model = class_svm()
fold = [inception_features_train, inception_features_test, superclass_labels_train, superclass_labels_test]
start = time.time()
model.train_hard_margin_svm(fold, gama)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))

"""# 1.6 Train a soft margin SVM with radial basis function (rbf) as kernel."""

# Commented out IPython magic to ensure Python compatibility.
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
cs = [1e-2, 1, 1e2]
model = class_svm()
result = model.stratified_k_fold(features=hog_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.train_soft_margin_svm_last(result[i], gamas , cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, C value: {} \n".format(gamas[ind % 3], cs[int(ind / 3)]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[ind % 3]]
c = [cs[int(ind / 3)]]
fold = [hog_features_train, hog_features_test, superclass_labels_train, superclass_labels_test]
model = class_svm()
start = time.time()
best = model.train_soft_margin_svm_last(fold, gama , c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
cs = [1e-2, 1, 1e2]
model = class_svm()
result = model.stratified_k_fold(features=inception_features_train, labels=superclass_labels_train, k=5)
f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.train_soft_margin_svm_last(result[i], gamas , cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)

avg_f1 = np.average(f1s, axis=0)
print(avg_f1)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, C value: {} \n".format(gamas[ind % 3], cs[int(ind / 3)]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[ind % 3]]
c = [cs[int(ind / 3)]]
fold = [inception_features_train, inception_features_test, superclass_labels_train, superclass_labels_test]
model = class_svm()
start = time.time()
best = model.train_soft_margin_svm_last(fold, gama , c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))

"""# Question 1.7
**Subclass Classification using SVM**
"""

inception_features_train = q1_dataset['inception_features_train']
inception_features_test = q1_dataset['inception_features_test']
hog_features_train = q1_dataset['hog_features_train']
hog_features_test = q1_dataset['hog_features_test']
subclass_labels_train = q1_dataset['subclass_labels_train']
subclass_labels_test = q1_dataset['subclass_labels_test']

from sklearn.model_selection import StratifiedKFold
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from statistics import mean


class subclass_prediction_svm:
    # def __init__():

    # Finding the test and train sets for stratified k fold !!
    def stratified_k_fold(self, features, labels, k):
        result = []
        skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)
        for train_index, test_index in skf.split(features, labels):
            # print("TRAIN:", len(train_index), "TEST:", len(test_index))
            X_train, X_test = features[train_index], features[test_index]
            y_train, y_test = labels[train_index], labels[test_index]
            result.append([X_train, X_test, y_train, y_test])

        return np.asarray(result)

    def multi_class_rbf_kernel(self, fold, gamas, cs):

        # Test and training data,label
        X_train, X_test, y_train, y_test = fold
        y_train = y_train.flatten()

        all_acc = []
        all_pred = []
        all_recall = []
        all_f1 = []

        for gama in gamas:
            for c in cs:
                # RBF Kernel
                clf = svm.SVC(kernel='rbf', gamma=gama, C=c)
                # Train the model using the training sets
                clf.fit(X_train, y_train)
                # Predict the response for test dataset
                y_pred = clf.predict(X_test)

                # Model Accuracy: how often is the classifier correct?
                acc = metrics.accuracy_score(y_test, y_pred)
                all_acc.append(acc)

                print("When gamma= {}, c = {}, Accuracy: {}".format(gama, c, acc))
            print()

        if len(gamas) != 1:
            return all_acc

        else:
            accuracy = accuracy_score(y_test, y_pred)
            print("Accuracy : {} ".format(accuracy))
            cnf_matrix = confusion_matrix(y_test, y_pred)
            print("Confusion Matrix : \n{} ".format(cnf_matrix))
            target_names = ['subclass 0', 'subclass 1', 'subclass 2', 'subclass 3', 'subclass 4', 'subclass 5',
                            'subclass 6', 'subclass 7', 'subclass 8', 'subclass 9']
            report = classification_report(y_test, y_pred, target_names=target_names)
            print(report)

    def multi_class_poly_kernel(self, fold, gamas, degrees):
        # Test and training data,label
        X_train, X_test, y_train, y_test = fold
        y_train = y_train.flatten()

        all_acc = []

        for gama in gamas:
            for deg in degrees:
                # RBF Kernel
                clf = svm.SVC(kernel='poly', degree=deg, gamma=gama, C=1e11)
                # Train the model using the training sets
                clf.fit(X_train, y_train)
                # Predict the response for test dataset
                y_pred = clf.predict(X_test)

                # Model Accuracy: how often is the classifier correct?
                acc = metrics.accuracy_score(y_test, y_pred)
                all_acc.append(acc)
                print("When gamma= {}, degree = {}, Accuracy: {}".format(gama, deg, acc))

            print()

        if len(gamas) != 1:
            return all_acc
        else:
            accuracy = accuracy_score(y_test, y_pred)
            print("Accuracy : {} ".format(accuracy))
            cnf_matrix = confusion_matrix(y_test, y_pred)
            print("Confusion Matrix : \n{} ".format(cnf_matrix))
            target_names = ['subclass 0', 'subclass 1', 'subclass 2', 'subclass 3', 'subclass 4', 'subclass 5',
                            'subclass 6', 'subclass 7', 'subclass 8', 'subclass 9']
            report = classification_report(y_test, y_pred, target_names=target_names)
            print(report)


"""# 1.7 Train a soft margin SVM with radial basis function(rbf) as kernel 
One-vs-all approach.
"""

# Commented out IPython magic to ensure Python compatibility.
cs = [1e-2, 1, 1e2]
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
model = subclass_prediction_svm()
result = model.stratified_k_fold(features=hog_features_train, labels=subclass_labels_train, k=5)

f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.multi_class_rbf_kernel (result[i], gamas , cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, C value: {} \n".format(gamas[int(ind / 3)], cs[ind % 3]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[int(ind / 3)]]
c = [cs[ind % 3]]
fold = [hog_features_train, hog_features_test, subclass_labels_train, subclass_labels_test]
model = subclass_prediction_svm()
start = time.time()
best = model.multi_class_rbf_kernel(fold, gama , c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
cs = [1e-2, 1, 1e2]
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
model = subclass_prediction_svm()
result = model.stratified_k_fold(features=inception_features_train, labels=subclass_labels_train, k=5)

f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.multi_class_rbf_kernel (result[i], gamas , cs)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, C value: {} \n".format(gamas[int(ind / 3)], cs[ind % 3]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[int(ind / 3)]]
c = [cs[ind % 3]]
fold = [inception_features_train, inception_features_test, subclass_labels_train, subclass_labels_test]
model = subclass_prediction_svm()
start = time.time()
best = model.multi_class_rbf_kernel(fold, gama , c)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# 1.8 Train a hard margin SVM with polynomial kernel."""

# Commented out IPython magic to ensure Python compatibility.
degrees = [3, 5, 7]
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
model = subclass_prediction_svm()
result = model.stratified_k_fold(features=hog_features_train, labels=subclass_labels_train, k=5)

f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.multi_class_poly_kernel (result[i], gamas , degrees)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, degree value: {} \n".format(gamas[int(ind / 3)], degrees[ind % 3]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[ind % 3]]
degree = [degrees[int(ind / 3)]]
fold = [hog_features_train, hog_features_test, subclass_labels_train, subclass_labels_test]
model = subclass_prediction_svm()
start = time.time()
best = model.multi_class_poly_kernel(fold, gama , degree)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# Doing everything for the neural network generated features"""

# Commented out IPython magic to ensure Python compatibility.
degrees = [3, 5, 7]
gamas = [2 ** -2, 2 ** 1, 2 ** 6]
model = subclass_prediction_svm()
result = model.stratified_k_fold(features=inception_features_train, labels=subclass_labels_train, k=5)

f1s = []
for i in range(len(result)):
    print('Fold {} is processing...'.format(i + 1))
    start = time.time()
    best = model.multi_class_poly_kernel (result[i], gamas , degrees)
    end = time.time()
    print("Total time elapsed: {} seconds ".format(end - start))
    f1s.append(best)
    print()

avg_f1 = np.average(f1s, axis=0)
ind_arr = np.where(avg_f1 == max(avg_f1))
ind = ind_arr[0][0]
print("Best of the Gamma value: {}, degree value: {} \n".format(gamas[int(ind / 3)], degree[ind % 3]))
print(avg_f1)

# Commented out IPython magic to ensure Python compatibility.
gama = [gamas[ind % 3]]
degree = [degrees[int(ind / 3)]]
fold = [inception_features_train, inception_features_test, subclass_labels_train, subclass_labels_test]
model = subclass_prediction_svm()
start = time.time()
best = model.multi_class_poly_kernel(fold, gama , degree)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# Question 2 - PCA

**First organizing the data**
"""

q2_dataset = loadmat(q2)
data = q2_dataset['data']
X = np.reshape(data, (150, 10625))
print(X.shape)

# imports
import numpy as np
from numpy import array
from numpy import mean
from numpy import cov
import numpy.linalg
import pandas as pd
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt


class pca_implementation:
    # def __init__():
    def cov_based_PCA(self, A):
        x_std = (A - A.mean())
        # features are columns from x_std
        covariance_matrix = np.cov(x_std)
        print("Calculating eigenvalues and eigenvalues..")

        eig_vals, eig_vecs = np.linalg.eig(covariance_matrix)
        '''
        print('\nEigenvectors\n%s' %eig_vecs)
        print('\nEigenvalues \n%s' %eig_vals)
        '''
        reconst = (np.mat(eig_vecs) * np.mat(x_std))

        result = pd.DataFrame(reconst)
        print("\Reconstructed image shape: {}\n".format(result.shape))
        result = np.real(result)

        mse = self.mean_squ_err(x_std, result)
        print("MSE for cov based implementation: {}\n".format(mse))

        return A, result

    def svd_based_PCA(self, A):
        print("Calculating Principal components with svd...")

        x_std = (A - A.mean())

        u, s, v = np.linalg.svd(x_std, False)

        k = u.shape[0]
        reconst = np.mat(u[:, :k]) * np.diag(s[:k]) * np.mat(v[:k, :])

        result = pd.DataFrame(reconst)
        print("\nReconstructed image shape: {}\n".format(result.shape))
        result = np.real(result)

        mse = self.mean_squ_err(A, result)
        print("MSE for svd based implementation: {}\n".format(mse))

        return A, result

    def mean_squ_err(self, orig, recons):
        # mse = (np.square(orig - recons)).mean()
        return mean_squared_error(orig, recons)

    def plot_res(self, original):
        first_ori = original[0:5, ]
        for i in first_ori:
            x = i.reshape(85, 125)
            self.display(x)

    def display(self, arr):
        fig, axs = plt.subplots(1, 1, figsize=(3, 5))
        axs.imshow(arr)
        plt.tight_layout()
        plt.show()


"""# 2.1 Finding MSE by SVD based implementation"""

# Commented out IPython magic to ensure Python compatibility.
model = pca_implementation()
start = time.time()
original1, reconst1 = model.svd_based_PCA(X)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# 2.2 Finding MSE by covariance based implementation"""

# Commented out IPython magic to ensure Python compatibility.
model = pca_implementation()
start = time.time()
original2, reconst2 = model.cov_based_PCA(X)
end = time.time()
print("Total time elapsed: {} seconds ".format(end-start))
"""# 2.3 Plot the following to a grid of size 3 x 5
for SVD result

**From Question 2.1**
"""

print("Printing original first 5 images from svd based implementation..")
model.plot_res(original1)

print("Printing reconstructed first 5 images from svd based implementation..")
model.plot_res(reconst1)

"""# 2.3 Plot the following to a grid of size 3 x 5
for covariance based implementation

**From Question 2.2**
"""

print("Printing original first 5 images from cov based implementation..")
model.plot_res(original2)

print("Printing reconstructed first 5 images from cov based implementation..")
model.plot_res(reconst2)